---
title : Bracket search
author : Michiel Stock
date: 2019-2020
---

```julia; echo=false
using Plots, LaTeXStrings
using STMO: myred, myblue, mygreen, myyellow, myorange,myblack
```

As a first introduction to mathematical optimization, we will study two *bracketing methods*. Bracketing methods can be used to minimize scalar function with a single input variable. These algorithms identify an interval $[a, b]$ containing the desired minimum. We will assume that the functions are *unimodal* they contain a single minimum.

# Bisection method

The bisection method is technically not a minimization algorithm, but a *root finding method*, i.e., it can be used to find an $x$ for a given function $g: \mathbb{R}\rightarrow \mathbb{R}$ such that

$$
g(x) = 0\,.
$$

We can use the bisection method to find a minimum of a function $f(x0$ by finding points where the first derivative $f'(x)$ is equal to 0.

> **Question**: in addition to the first derivative being equal to zero, which other criterion should hold for $x^\star$ to be a minimizer?

The bisection method departs from an inital bracket $[a, b]$, where $f(a)$ and $f(b)$ have opposing signs. If $f'(x)$ is a continious function, the *intermediate value theorem* states that there is at least one $x\in[a,b]$ such that $f'(x)$.

In every step of the bisection method, the interval it cut into half. The midpoint $x = (b-a)/2$ is computed, and a new bracket is formed from the midpoint and the side that contains zero.

```julia; echo=false
g(x) = tanh(sin(x))

a, b = -0.75, 1.2

plot(g, -pi/2:0.1:pi/2, label="\$g(x)\$", lw=2, color=myblue, xlabel="\$ x\$", legend=:left)
scatter!([a], [g(a)], color=myorange, label=:a)
scatter!([b], [g(b)], color=mygreen, label=:b)
vline!([(b+a)/2], color=myred, label="\$x = (b-a)/2\$", lw=2)
hline!([0], label="", color=myblack, ls=:dash)
title!("Illustration of the bisection method")
```

The bisection procedure is repeated until the length of the interval is smaller than a small $\epsilon > 0$. This $\epsilon$ is called a *convergence parameter*, because it determines when the algorithm will halt. The pseudocode of the bisection method is given below.

> **given** $f'(x)$, the derivative of a function, initial interval $[a, b]$, tollerance $\epsilon$.
>
> **while** $b - a > \epsilon$
>> 1. *Determine midpoint*. $x:=(a+b)/2$
>> 2. *Update*.
>>> **if $f'(x)=0$**: $a:=x$, $b:=x$
>>> **else if $\text{sign}(f'(x)) = \text{sign}(f'(a))$**: $a:=x$
>>> **else**: $b:=x$
>
> **Output**: $[a, b]$

Since every step shrinks the interval with a factor two, it is easy to show that this procedure stops within

$$
\log_2(\frac{b-a}{\epsilon})
$$

iterations.

**Assignment 1** Complete the code for the bisection method. Use it to find the minimum of

$$
f(x) = \log(e^{-x} + \frac{x^2}{2})
$$

Check your result graphically.

```julia; eval=false
function bisection(f′, a, b; ϵ=1e-3)
  @assert a < b, "a should be smaller than b"

  # a is minizer?
  f′(a) == 0 && return a, a
  # b is minizer?
  f′(b) == 0 && return b, b

  while ...
    ...
  end
  return a, b
end
```

```julia
f(x) = log(exp(-x) + 0.5x^2)
```
```julia; eval=false
f′(x) = ...
```

```julia; eval=false

```

# Quadratic fit search

As we will see in the next lecture, finding the minimum of quadratic functions of the form

$$
q(x) = p_1 + p_2 x + p_3 x^2
$$

is trivial. The *quadratic fit search method* will approximate a function by a quadratic function and computes the minimizer of that function.

Given three bracketing points $a<b<c$ and their resprective evaluations $y_a=f(a), y_b=f(b)$ and $y_c=f(c)$, we can fit a quadratic curve by solving the following system of equations for the coefficients:

$$
\begin{bmatrix}y_a\\ y_b \\ y_c\end{bmatrix} = \begin{bmatrix}1 & a & a^2 \\ 1 & b & b^2 \\ 1 & c & c^2 \end{bmatrix} \begin{bmatrix}p_1\\ p_2 \\ p_3\end{bmatrix}\,.
$$

For example, in Julia, we have:

```julia
a, b, c = -2, -1, 5
ya, yb, yc = f.([a, b, c])

p1, p2, p3 = [1 a a^2; 1 b b^2; 1 c c^2] \ [ya, yb, yc]
```

We can even dispense solving the system and use a closed-form fit based on three points:

$$
q(x) = y_a\frac{(x-b)(x-c)}{(a-b)(a-c)} +  y_b\frac{(x-a)(x-c)}{(b-a)(b-c)}+ y_c\frac{(x-a)(x-b)}{(c-a)(c-b)}\,.
$$

The unique minimum of this quadratic curve is computed by:

$$
x^\star = \frac{1}{2} \frac{y_a(b^2-c^2) + y_b(c^2-a^2)+ y_c(a^2-b^2)}{y_a(b-c) + y_b(c-a)+ y_c(a-b)}\,.
$$

We have implemented these two formulas in `quadratic_fit` and `quadratic_fit_min`. See below for an illustration.

```julia
function quadratic_fit(a, b, c, ya, yb, yc)
  @assert a != b != c
  return x -> ya * (x-b) * (x-c)/(a-b)/(a-c) +
    yb * (x-a)*(x-c)/(b-a)/(b-c) +
    yc * (x-a)*(x-b)/(c-a)/(c-b)
end

function quadratic_fit_min(a, b, c, ya, yb, yc)
  return 0.5 * (ya*(b^2-c^2) + yb*(c^2-a^2) + yc*(a^2 + b^2)) \
        (ya*(b-c) + yb*(c-a) + yc*(a + b))
end
```
```julia
x = quadratic_fit_min(a, b, c, ya, yb, yc)
```

```julia
plot(f, -5:0.1:7, label="\$f(x)\$", lw=2, xlabel="\$x\$", color=myblue)
scatter!([a, b, c], [ya, yb, yc], label="\$a, b, c\$", color=myorange)
plot!(quadratic_fit(a, b, c, ya, yb, yc), -5:0.1:7, label="quadratic fit \$q(x)\$", lw=2, color=mygreen)
vline!([x], label="\$x\$", lw=2, color=myred)
```

Given the minimizer $x^\star$ of the quadratic approximation, we can update the interval closer to this value. The quadratic fit search repeats this approach $n$ times. The pseudocode is given below.

> **given** $f(x)$, the function to be minimized, three increasing values $a, b, c$ with $f(a) > f(b)$ and $f(c) > f(b)$, the number of steps $n$.
>
> **repeat** $n$ times
>> 1. *Fit quadratic*: and set $x$ as its minimizer
>> 2. *Update bracket*:
>>> **if $x\in [a,b]$**: $a, b, c := a, x, b$
>>> **else**: $a, b, c := b, x, c$
>
> **Output**: $a, b, c$

**Assignment 2** Complete the code for the quadratic fit search method. Use it again to find the minimizer of the provided $f(x)$.

```julia; eval=false
function quadratic_fit_search(f, a, b, c, n)
  @assert a < b < c, "a, b, c not consecutive "
  @assert f(a) > f(b) && f(c) > f(b), "no local minimum between a and b"

  for i in 1:n
    ...
    ...
  end
  return a, b, c
end
```

```julia

```

```julia

```

The coming chapters will discuss the minimization of quadratic function in much greater detail.

# References

- Kochenderfer, M. J. and Wheeler, T., '*Algorithms for Optimization*'. MIT Press (2019)
