{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Numeric and automatic differentiation\n### Michiel Stock\n### 2019-2020"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots, BenchmarkTools, LaTeXStrings\nusing STMO"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivation\n\nUp to now, we confidently assumed that we would always be able to compute the derivative or gradient of any function. Despite differentiation being a relatively easy operation, it is frequenty not feasible (or desirable) to compute this by hand. *Numerical differentiation* can provide approximations of th derivate or gradient at a particular point. *Automatic differentiation* directly manipulates the computational graph to generate a function that computes the (exact) derivate. Such methods have advanced greatly in the last years and it is no exageration that their easy use in popular software libraries such as TenserFlow and PyTorch are a cornerstone of deep learning and other machine learning and scientific computing fields.\n\n# Definition of a derivative"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots, BenchmarkTools\nusing STMO"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivation\n\nUp to now, we confidently assumed that we would always be able to compute the derivative or gradient of any function. Despite differentiation being a relatively straightforward operation, it is frequently not feasible (or desirable) to calculate this by hand. *Numerical differentiation* can provide approximations of the derivate or gradient at a particular point. *Automatic differentiation* directly manipulates the computational graph to generate a function that computes the (exact) derivate. Such methods have advanced dramatically in the last years, and it is no exaggeration that their easy use in popular software libraries such as TensorFlow and PyTorch is a cornerstone of deep learning and other machine learning and scientific computing fields.\n\n# Definition of a derivative\n\n$$\n\\frac{\\text{d}f(x)}{\\text{d}x} = f'(x) = \\lim _{h\\to 0}{\\frac {f(x+h)-f(x)}{h}}.\n$$\n\nWhen we work with function of several variables, we use *partial derivatives* (e.g. $\\frac{\\partial f(x, y)}{\\partial x}$), indicating we keep all variables but $x$ fixed.\n\nOur running example:\n\n$$\nf(x) = \\log x + \\frac{\\sin x}{x}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = log(x) + sin(x) / x;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Symbolic differentiation\n\nComputing derivatives, as you have seen in basic calculus courses.\n\nBy hand or automatically:\n- Maple\n- Sympy (python)\n- Mathematica\n- Maxima\n\nDifferentiation is *easy* compared to *integration* or *sampling*.\n\nAdvantages:\n- exact derivatives!\n- gives the formula for different evaluations.\n- no hyperparameters or tweaking: just works!\n\nDisadvantages:\n- some software not flexible enough (gradients, arrays, for-loops,...)\n- sometimes explosion of terms: *expression swell*\n- not always numerically optimal!"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SymEngine"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x  # define variable"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "df = diff(f(x), x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "df(2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(f, 1:0.01:5, label=\"\\$f(x)\\$\", xlabel=\"\\$x\\$\", lw=2, color=mygreen)\nplot!(df, 1:0.01:5, label=\"\\$f'(x)\\$\", lw=2, color=myorange)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numerical differentiation\n\nFinite difference approximation of the derivative/gradient based on a number of function evaluations.\n\nOften based on the limit definition of a derivative. Theoretical analysis using Taylor approximation:\n\n$$\nf(x + h) = f(x) + \\frac{h}{1!}f'(x) + \\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f^{(3)}(x)+\\ldots\n$$\n\n**Divided difference**\n\n$$\nf'(x)\\approx \\frac{f(x+h) - f(x)}{h}\n$$\n\n**Center difference**\n\n$$\nf'(x)\\approx \\frac{f(x+h) - f(x-h)}{2h}\n$$\n\n**Complex step method**\n\n$$\nf'(x)\\approx \\frac{\\text{Im} (f(x +ih))}{h}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "diff_divdiff(f, x; h=1e-10) = (f(x + h) - f(x)) / h;\ndiff_centdiff(f, x; h=1e-10) = (f(x + h) - f(x - h)) / 2h;\ndiff_complstep(f, x; h=1e-10) = imag(f(x + im * h)) / h;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime diff_divdiff($f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime diff_centdiff($f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime diff_complstep($f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First sin of numerical analysis**:\n\n> *thou shalt not add small numbers to big numbers*\n\n**second sin of numerical analysis**:\n\n> *thou shalt not subtract numbers which are approximately equal*"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fexamp(x) = 64x*(1-x)*(1-2x)^2*(1-8x+8x^2)^2\ndfexamp = diff(fexamp(x), x)\nerror(diff, h; x=1.0) = max(abs(Float64(dfexamp(x)) - diff(fexamp, x, h=h)), 1e-50);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "stepsizes = map(t->10.0^t, -20:0.1:-1);\nplot(stepsizes, error.(diff_divdiff, stepsizes), label=\"divided difference\",\n    xscale=:log10, yscale=:log10, lw=2, legend=:bottomright, color=myblue)\nplot!(stepsizes, error.(diff_centdiff, stepsizes), label=\"center difference\", lw=2,\n            color=myred)\nplot!(stepsizes, error.(diff_complstep, stepsizes), label=\"complex step\", lw=2,\n            color=myyellow)\n#xlims!(1e-15, 1e-1)\nxlabel!(\"\\$h\\$\")\nylabel!(\"absolute error\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages of numerical differentiation:\n- easy to implement\n- general, no assumptions needed\n\nDisadvantages:\n- not numerically stable (round-off errors)\n- not efficient for gradients ($\\mathcal{O}(n)$ evaluations for $n$-dimensional vectors)\n\n\n## Approximations of multiplications with gradients\n\n**Gradient-vector approximation**\n\n$$\n\\nabla f(\\mathbf{x})^\\intercal \\mathbf{d} \\approx \\frac{f(\\mathbf{x}+h\\cdot\\mathbf{d}) - f(\\mathbf{x}-h\\cdot\\mathbf{d})}{2h}\n$$\n\n**Hessian-vector approximation**\n\n$$\n\\nabla^2 f(\\mathbf{x}) \\mathbf{d} \\approx \\frac{\\nabla f(\\mathbf{x}+h\\cdot\\mathbf{d}) - \\nabla f(\\mathbf{x}-h\\cdot\\mathbf{d})}{2h}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "grad_vect(f, x, d; h=1e-10) = (f(x + h * d) - f(x - h * d)) / (2h)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dvect = randn(10) / 10\nxvect = 2rand(10)\n\nA = randn(10, 10)\nA = A * A' / 100\n\n#g(x) = exp(- x' * A * x)  # adjoint does not play with Zygote\ng(x) = exp(- sum(x .* (A * x)))\n\n# correct gradient and Hessian (by hand)\nDg(x) = -2g(x) * A * x\nD²g(x) = -2g(x) * A - 2A * x * Dg(x)'"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "g(xvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Dg(xvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Dg(xvect)' * dvect"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "grad_vect(g, xvect, dvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "D²g(xvect) * dvect"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "h = 1e-10\n(Dg(xvect + h * dvect) - Dg(xvect - h * dvect)) / 2h"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward differentiation\n\nAccumulation of the gradients along the *computational graph*.\n\n<img src=\"Figures/compgraph.png\" alt=\"drawing\" width=\"400\"/>\n\nForward differentiation computes the gradient from the inputs to the outputs.\n\n## Differentiation rules\n\n**Sum rule**:\n\n$$\n\\frac{\\partial (f(x)+g(x))}{\\partial x} =  \\frac{\\partial f(x)}{\\partial x} + \\frac{\\partial f(x)}{\\partial x}\n$$\n\n**Product rule**:\n\n$$\n\\frac{\\partial (f(x)g(x))}{\\partial x} =  f(x)\\frac{\\partial g(x)}{\\partial x} + g(x)\\frac{\\partial f(x)}{\\partial x}\n$$\n\n**Chain rule**:\n\n$$\n\\frac{\\partial (g(f(x))}{\\partial x} =  \\frac{\\partial g(u)}{\\partial u}\\mid_{u=f(x)} \\frac{\\partial f(x)}{\\partial x}\n$$\n\n## Example of the forward differentiation\n\n<img src=\"Figures/forwarddiff.png\" alt=\"drawing\" width=\"600\"/>\n\n## Dual numbers\n\nForward differentiation can be viewed as evaluating function using *dual numbers*, which can be viewed as truncated Taylor series:\n\n$$\nv + \\dot{v}\\epsilon\\,,\n$$\n\nwhere $v,\\dot{v}\\in\\mathbb{R}$ and $\\epsilon$ a nilpotent number, i.e. $\\epsilon^2=0$. For example, we have\n\n$$\n(v + \\dot{v}\\epsilon) + (u + \\dot{u}\\epsilon) = (v+u) + (\\dot{v} +\\dot{u})\\epsilon\n$$\n\n\n$$\n(v + \\dot{v}\\epsilon)(u + \\dot{u}\\epsilon) = (vu) + (v\\dot{u} +\\dot{v}u)\\epsilon\\,.\n$$\n\n\nThese dual numbers can be used as\n\n$$\nf(v+\\dot{v}\\epsilon) = f(v) + f'(v)\\dot{v}\\epsilon\\,.\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "struct Dual\n    v\n    vdot\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Base.:+(a::Dual, b::Dual) = Dual(a.v + b.v, a.vdot + b.vdot)\nBase.:*(a::Dual, b::Dual) = Dual(a.v * b.v, a.v * b.vdot + b.v * a.vdot)\nBase.:*(v::Real, b::Dual) = Dual(v, 0.0) * b\nBase.:sin(a::Dual) = Dual(sin(a.v), cos(a.v) * a.vdot)\nBase.:log(a::Dual) = Dual(log(a.v), 1.0 / a.v * a.vdot)\nBase.:/(a::Dual, b::Dual) = Dual(a.v / b.v, (a.vdot * b.v - a.v * b.vdot) / b.v^2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime f(Dual(2.0, 1.0))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "myforwarddiff(f, x) = f(Dual(x, 1.0)).vdot\n\n@btime myforwarddiff($f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This directly works for vectors!"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q(x) = 10.0 * x[1] * x[2] + x[1] * x[1] + sin(x[1]) / x[2]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q([1, 2])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q(Dual.([1, 2], [1, 0]))  # partial wrt x1"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q(Dual.([1, 2], [0, 1]))  # partial wrt x2"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using ForwardDiff"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime ForwardDiff.derivative($f, 2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime ForwardDiff.gradient($g, $xvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ForwardDiff.gradient(q, [1, 2])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward differentiation:\n\n- exact gradients!\n- computational complexity scales with **number of inputs**\n- used when you have more outputs than inputs\n\n# Reverse differentiation\n\nCompute the gradient from the output toward the inputs using the chain rule.\n\n<img src=\"Figures/reversediff.png\" alt=\"drawing\" width=\"600\"/>\n\nReverse differentiation:\n\n- also exact!\n- main workhorse for training artificial neural networks.\n- efficient when more inputs than outputs (machine learning: thousands of parameters vs. one loss)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Zygote"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime f'(2.0)  # that's it"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime g'(xvect)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-layer perceptron.\n\n<img src=\"Figures/ANN_example.png\" alt=\"drawing\" width=\"200\"/>\n\nReverse differentation or backpropagation.\n\n<img src=\"Figures/Forwardprop.png\" alt=\"drawing\" width=\"500\"/>\n\n\nReverse differentation or backpropagation.\n\n<img src=\"Figures/Backprop.png\" alt=\"drawing\" width=\"500\"/>\n\nReturns effect of changing layer output on the loss. Can be related directly to the parameters!\n\n## Exercise: logistic regression\n\nRecall logistic regression on a training set $S=\\{(\\mathbf{x}_i, y_i)\\mid i=1,\\ldots,n\\}$ with $y\\in\\{0,1\\}$.\n\nPrediction:\n\n$$\nf(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\intercal\\mathbf{x})\\,,\n$$\n\nwith $\\sigma(t) = 1 /(1+exp(t))$.\n\nTo find the parameter vector $\\mathbf{w}$, we minimize the cross-entropy:\n\n$$\nL(\\mathbf{w};S)= \\sum_{i=1}^n = - y_i \\log(f(\\mathbf{x})) - (1-y_i)\\log(1-f(\\mathbf{x}))\\,.\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# artificial data\nX = [randn(50, 2); randn(50, 2) .+ [-1.0 2.4]];\ny = [i <= 50 ? 0 : 1 for i in 1:100];\nn = length(y);\n\nscatter(X[:,1], X[:,2], color=y)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "σ(t) = 1.0 / (1.0 + exp(t))\nf(x, w) = σ(sum(x .* w))\nL(w; X=X, y=y) = sum(- y .* log.(σ.(X * w)) - (1.0 .- y) .* log.(σ.(1. .- X * w)))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "w = [0.1, 0.1]\nL(w)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignments**\n\n1. Compute the gradient of $L$ w.r.t. $\\mathbf{w}$ using\n    - numerical method\n    - forward differentiation\n    - backward differentiation\n2. (optional) Implement a simple gradient descent to find  $\\mathbf{w}^\\star$.\n3. Add a bias to the prediction function.\n\n\n# Differentiating ODE\n\nAutomatic differentiation can be used beyond machine learning and optimization:\n\n- [physical engines](https://arxiv.org/abs/1611.01652) to learn robot control\n- differentiating [protein](https://github.com/lupoglaz/TorchProteinLibrary) [structures](https://www.cell.com/cell-systems/fulltext/S2405-4712(19)30076-6)\n- Sinkhorn algorithm\n- [dynamic programming](https://arxiv.org/abs/1802.03676)\n- [differential equations](https://julialang.org/blog/2019/01/fluxdiffeq)\n\nEverything is computed by some straightforward and differentiable functions!\n\n## Short introduction to differential equations\n\nOrdinary differential equations describe dynamical processes through the *rate of change*:\n\n$$\n\\dot{\\mathbf{x}} = f(\\mathbf{x}, t; \\theta)\\,,\n$$\n\nwith $\\theta$ some parameters. To obtain a unique solution, we depart from some initial vector for $\\mathbf{x}$ at the starting time.\n\nUsually solved using an iterative algorithm, for example using Euler's method:\n\n$$\n\\mathbf{x}_{t+1} = \\mathbf{x}_{t} + \\alpha_t f(\\mathbf{x}_{t}, t)\\,.\n$$\n\nwith $\\alpha_t$ a stepsize (does this remind you of something?).\n\n## Example an auto-catalytic process\n\nConsider a chemical reaction where some molecule $X$ combines reversibly with a molecule $A$ to form two molecules $X$:\n\n$$\nX+A    \\rightleftharpoons 2X\\,.\n$$\n\nWe assume that there is a large and inexhaustible supply of $A$. Based on the *law of mass action* for chemical kinetics, the concentration of $X$ is given by\n\n$$\n\\dot{x} = k_1ax - k_{-1} x^2\\,,\n$$\n\nwith\n-  $x$ the concentration of $X$;\n-  $a$ the concentration of $A$;\n-  $k_1$ and $k_{-1}$ the forward and backward kinetic parameters."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function eulersolve(f, x0, (t0, tend), θ, α=0.1)\n    tsteps = t0:α:tend\n    # demonstration for didactical purposes\n    # Any arrays are generally a bad idea!\n    sol = Array{eltype(x0)}(undef, length(tsteps), length(x0))\n    x = x0\n    for (i, t) in enumerate(tsteps)\n        sol[i, :] .= x\n        x += α * f(x, t, θ)\n    end\n    return sol\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fautocat(x, t, (a, k₁, k₋₁)) = k₁ * a * x - k₋₁ * x.^2"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "solution = eulersolve(fautocat, [10.0], (0.0, 10.0), (5, 0.1, 0.01))\ntsteps = 0:0.1:10\nplot(tsteps, solution, label=\"x\", lw=2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignments**\n\n1. Given $x_0=10$ and $(a, k_1, k_{-1}) = (5, 0.1, 0.01)$, compute the gradient of $x(t)$ w.r.t. $x_0$ (e.g. sensitivity for initial conditions.\n2. Given these parameters, use gradient descent to tune $x_0$ such that $x(10) = 20$.\n3. Given a vector of measured concentrations with $x_0=10$, find the parameters of the process through gradient descent."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# for the first two questions\nxfromx0(x0) = eulersolve(fautocat, x0, (0.0, 10.0), (5, 0.1, 0.01))\nx10fromx0(x0) = xfromx0(x0)[end]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n\nConsider the *Wheeler's Ridge* function:\n\n$$\nf(\\mathbf{x}) = -\\exp(-(x\\_1 x\\_2 - a)^2 -(x^2 -a)^2)\\,,\n$$\n\nat the point $\\mathbf{x}_0=[1.5, 1.5]^T$. We set $a=1.5$.\n\nImplement this function."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the gradient by hand.\n\nFind the gradient and Hessian at $\\mathbf{x}_0$ by numerical differentiation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the gradient and Hessian at $\\mathbf{x}_0$ using automatic differentiation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n\n- Gunes et. al. (2015) *Automatic differentiation in machine learning: a survey*\n- Kochenderfer, M. J. and Wheeler, T., '*Algorithms for Optimization*'. MIT Press (2019)"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.3.1"
    },
    "kernelspec": {
      "name": "julia-1.3",
      "display_name": "Julia 1.3.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
